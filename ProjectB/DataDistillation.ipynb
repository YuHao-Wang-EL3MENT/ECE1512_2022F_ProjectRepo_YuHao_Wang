{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16649,"status":"ok","timestamp":1670453857361,"user":{"displayName":"YuHao Wang","userId":"12192957869195443046"},"user_tz":300},"id":"wEm8seTgSICm","outputId":"f2711eb5-bb87-43ff-d7bb-32b6a99e06cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1808,"status":"ok","timestamp":1670453861481,"user":{"displayName":"YuHao Wang","userId":"12192957869195443046"},"user_tz":300},"id":"bJfoSD1dTVu8","outputId":"8640916b-f455-4436-9127-e4a27a746bc6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ECE1512/ProjectB/All_Code\n"]}],"source":["%cd /content/drive/MyDrive/ECE1512/ProjectB/All_Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1670453863162,"user":{"displayName":"YuHao Wang","userId":"12192957869195443046"},"user_tz":300},"id":"D7yR03TLTbB7","outputId":"6582ab02-055c-4af7-ed1a-ea0a7be074b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ECE1512/ProjectB/All_Code\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFKz_JegTdGi","outputId":"f7b90b58-3183-4262-e2c1-e26136b6516b"},"outputs":[{"output_type":"stream","name":"stdout","text":["eval_it_pool:  [0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000]\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Experiment start!!!!!!!!!!!!!!!!!!!!!!\n","Hyper-parameters: \n"," {'method': 'DC', 'dataset': 'CIFAR10', 'model': 'LeNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 5, 'epoch_eval_train': 100, 'Iteration': 10000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 100, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7faa1ed67430>, 'dsa': False}\n","Evaluation model pool:  ['LeNet']\n","class c = 0: 5000 real images\n","class c = 1: 5000 real images\n","class c = 2: 5000 real images\n","class c = 3: 5000 real images\n","class c = 4: 5000 real images\n","class c = 5: 5000 real images\n","class c = 6: 5000 real images\n","class c = 7: 5000 real images\n","class c = 8: 5000 real images\n","class c = 9: 5000 real images\n","real images channel 0, mean = -0.0000, std = 1.2211\n","real images channel 1, mean = -0.0002, std = 1.2211\n","real images channel 2, mean = 0.0002, std = 1.3014\n","main.py:91: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n","  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n","main.py:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n","initialize synthetic data from random noise\n","[2022-12-07 04:19:34] The synthetic samples are generated and training will begin!!!!!!!!!\n","[2022-12-07 04:19:39] iter = 0000, loss = 107.2453\n"]}],"source":["!python main.py \\\n","    --dataset 'CIFAR10' \\\n","    --Iteration 10000 "]},{"cell_type":"code","source":["!python main.py \\\n","    --dataset 'MNIST' \\\n","    --Iteration 10000 "],"metadata":{"id":"EU-qXEDtzRQN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#MHIST Dataset"],"metadata":{"id":"vh265fW4jv2E"}},{"cell_type":"code","source":["!python main.py \\\n","    --dataset 'MHIST' \\\n","    --Iteration 1000 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_vpkIEzsi4C","executionInfo":{"status":"ok","timestamp":1670454102434,"user_tz":300,"elapsed":21282,"user":{"displayName":"YuHao Wang","userId":"12192957869195443046"}},"outputId":"90a221ea-e26f-444a-8468-8af611cb21ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["eval_it_pool:  [0, 500, 1000]\n","tcmalloc: large alloc 2619187200 bytes == 0x41692000 @  0x7f1901cb51e7 0x7f18ff5bf14e 0x7f18ff617745 0x7f18ff617878 0x7f18ff6d7604 0x7f18ff6da8ec 0x7f18ff865bd4 0x4eb089 0x5d86fe 0x7f18ff6e0e6b 0x606ad6 0x55ff12 0x5d7c18 0x49ec69 0x55e858 0x5d7cf1 0x49ec69 0x55e571 0x55ef23 0x642140 0x6421be 0x644688 0x644c2c 0x6776be 0x677889 0x7f19018b2c87 0x5e0f1a\n","/content/drive/MyDrive/ECE1512/ProjectB/All_Code/utils.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  mean = torch.tensor(mhist_train_img, dtype=torch.float32)\n","tcmalloc: large alloc 1309597696 bytes == 0x123a72000 @  0x7f1901c97b6b 0x7f1901cb7379 0x7f187b3ae935 0x7f187b38c093 0x7f18a84604ca 0x7f18a8458603 0x7f18a845865a 0x7f18a84586bf 0x7f18a8afb469 0x7f18a954a7b0 0x7f18a954a810 0x7f18a920ab33 0x7f18a951d31e 0x7f18a924ddeb 0x7f18a8784ec7 0x7f18a8aec13b 0x7f18a96d7b3b 0x7f18a8f25425 0x7f18a951d093 0x7f18a8f25425 0x7f18aa8dda0b 0x7f18aa8dde7e 0x7f18a8fa5989 0x7f18a8ae4517 0x7f18a9895f79 0x7f18a910ce8a 0x7f18d14ce0dd 0x7f18d14d3606 0x7f18d110425f 0x5d746e 0x5d813c\n","/content/drive/MyDrive/ECE1512/ProjectB/All_Code/utils.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  std = torch.tensor(mhist_train_label_le)\n","class c = 0: 1545 real images\n","class c = 1: 630 real images\n","real images channel 0, mean = 0.7502, std = 0.2134\n","real images channel 1, mean = 0.7044, std = 0.2104\n","real images channel 2, mean = 0.7071, std = 0.2113\n","main.py:109: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n","  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n","main.py:109: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n","  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n","initialize synthetic data from random noise\n","[2022-12-07 23:01:37] The synthetic samples are generated and training will begin!!!!!!!!!\n","Traceback (most recent call last):\n","  File \"main.py\", line 261, in <module>\n","    main()\n","  File \"main.py\", line 212, in main\n","    output_real = net(img_real)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/drive/MyDrive/ECE1512/ProjectB/All_Code/networks.py\", line 162, in forward\n","    x = self.features(x)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 463, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.30 GiB (GPU 0; 14.76 GiB total capacity; 12.10 GiB already allocated; 1.61 GiB free; 12.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPvwbCWUsIPD83x6SoHYs8e"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}